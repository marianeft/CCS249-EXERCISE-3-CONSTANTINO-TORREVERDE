{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9lJpRhYMd54WHfXg5u4xa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marianeft/CCS249-EXERCISE-3-CONSTANTINO-TORREVERDE/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLXubtD--Zl4"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki = wikipediaapi.Wikipedia('en')\n",
        "\n",
        "# Fetch Wikipedia Page\n",
        "topic = \"Artificial Intelligence\"  # Replace this with your topic\n",
        "page = wiki.page(topic)\n",
        "if not page.exists():\n",
        "    print(\"Topic does not exist on Wikipedia\")\n",
        "    exit()\n",
        "\n",
        "# Get the text and truncate to 1000 words\n",
        "corpus = ' '.join(page.text.split()[:1000])  # Limit to 1000 words\n",
        "print(\"Corpus Loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bigram_model(corpus):\n",
        "    tokens = word_tokenize(corpus.lower())  # Tokenize and lowercase\n",
        "    bigram_counts = Counter(bigrams(tokens))\n",
        "    unigram_counts = Counter(tokens)\n",
        "\n",
        "    # Compute probabilities\n",
        "    bigram_probs = {bigram: count / unigram_counts[bigram[0]]\n",
        "                    for bigram, count in bigram_counts.items()}\n",
        "\n",
        "    return bigram_probs\n",
        "\n",
        "bigram_model = train_bigram_model(corpus)\n",
        "print(\"Bigram Model Trained!\")"
      ],
      "metadata": {
        "id": "CjA9HeTZARWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_trigram_model(corpus):\n",
        "    tokens = word_tokenize(corpus.lower())  # Tokenize and lowercase\n",
        "    trigram_counts = Counter(trigrams(tokens))\n",
        "    bigram_counts = Counter(bigrams(tokens))\n",
        "\n",
        "    # Compute probabilities\n",
        "    trigram_probs = {trigram: count / bigram_counts[(trigram[0], trigram[1])]\n",
        "                     for trigram, count in trigram_counts.items()}\n",
        "\n",
        "    return trigram_probs\n",
        "\n",
        "trigram_model = train_trigram_model(corpus)\n",
        "print(\"Trigram Model Trained!\")\n"
      ],
      "metadata": {
        "id": "_3efoDClAcbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, ngram_type, test_sentence):\n",
        "    tokens = word_tokenize(test_sentence.lower())\n",
        "    ngrams = list(ngram_type(tokens))\n",
        "    log_prob = 0.0\n",
        "    for ngram in ngrams:\n",
        "        prob = model.get(ngram, 1e-6)  # Assign a small probability if unseen\n",
        "        log_prob += -math.log(prob)\n",
        "    perplexity = math.exp(log_prob / len(ngrams))\n",
        "    return perplexity\n",
        "\n",
        "test_sentence = \"The quick brown fox jumps over the lazy dog near the bank of the river.\"\n",
        "\n",
        "bigram_perplexity = calculate_perplexity(bigram_model, bigrams, test_sentence)\n",
        "trigram_perplexity = calculate_perplexity(trigram_model, trigrams, test_sentence)\n",
        "\n",
        "print(f\"Bigram Model Perplexity: {bigram_perplexity}\")\n",
        "print(f\"Trigram Model Perplexity: {trigram_perplexity}\")"
      ],
      "metadata": {
        "id": "KEmA72zrAdsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}